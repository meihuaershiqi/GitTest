{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from tensorflow.keras import Model, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/cnews.train.txt\"\n",
    "\n",
    "MAXLEN = 600\n",
    "BATCH_SIZE = 128\n",
    "embedding_dims = 50\n",
    "EPOCHS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZCF\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.149 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 5000 条！！！\n",
      "已处理 10000 条！！！\n",
      "已处理 15000 条！！！\n",
      "已处理 20000 条！！！\n",
      "已处理 25000 条！！！\n",
      "已处理 30000 条！！！\n",
      "已处理 35000 条！！！\n",
      "已处理 40000 条！！！\n",
      "已处理 45000 条！！！\n",
      "已处理 50000 条！！！\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  文本的类别及其对应id的字典\n",
    "categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "cat_to_id = dict(zip(categories, range(len(categories)))) \n",
    "\n",
    "contents, labels = [], []\n",
    "with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            label, content = line.strip().split('\\t')\n",
    "            if content:\n",
    "                contents.append(list(jieba.cut(content)))\n",
    "                labels.append(cat_to_id.get(label))\n",
    "                if len(labels)%5000==0:\n",
    "                    print(f'已处理 {len(labels)} 条！！！')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将contents中的每个词转换为数字列表，使用每个词的编号进行编号\n",
    "lang_tokenizer = Tokenizer(filters='')\n",
    "lang_tokenizer.fit_on_texts(contents)\n",
    "\n",
    "# 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "contents_tensor = lang_tokenizer.texts_to_sequences(contents)\n",
    "contents_tensor = pad_sequences(contents_tensor, maxlen=MAXLEN)\n",
    "\n",
    "# 数据\n",
    "X = contents_tensor\n",
    "Y = utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "x_train ,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "vocab_size = len(lang_tokenizer.index_word)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. TextCNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 maxlen,\n",
    "                 vocab_size ,\n",
    "                 embedding_dims,\n",
    "                 kernel_sizes=[3, 4, 5],\n",
    "                 class_num=10,\n",
    "                 last_activation='softmax'):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "        self.embedding = Embedding(self.vocab_size, self.embedding_dims, input_length=self.maxlen)\n",
    "        self.convs = []\n",
    "        self.max_poolings = []\n",
    "        for kernel_size in self.kernel_sizes:\n",
    "            self.convs.append(Conv1D(128, kernel_size, activation='relu'))\n",
    "            self.max_poolings.append(GlobalMaxPooling1D())\n",
    "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedding = self.embedding(inputs)\n",
    "        convs = []\n",
    "        for i in range(len(self.kernel_sizes)):\n",
    "            c = self.convs[i](embedding)\n",
    "            c = self.max_poolings[i](c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/7\n",
      "40000/40000 [==============================] - 302s 8ms/sample - loss: 0.1131 - accuracy: 0.9620 - val_loss: 0.0278 - val_accuracy: 0.9906\n",
      "Epoch 2/7\n",
      "40000/40000 [==============================] - 303s 8ms/sample - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0187 - val_accuracy: 0.9938\n",
      "Epoch 3/7\n",
      "40000/40000 [==============================] - 319s 8ms/sample - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0162 - val_accuracy: 0.9945\n",
      "Epoch 4/7\n",
      "40000/40000 [==============================] - 309s 8ms/sample - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0156 - val_accuracy: 0.9947\n",
      "Epoch 5/7\n",
      "40000/40000 [==============================] - 315s 8ms/sample - loss: 7.3161e-04 - accuracy: 0.9999 - val_loss: 0.0157 - val_accuracy: 0.9949\n",
      "Epoch 6/7\n",
      "40000/40000 [==============================] - 309s 8ms/sample - loss: 3.0008e-04 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 0.9948\n",
      "Epoch 7/7\n",
      "40000/40000 [==============================] - 298s 7ms/sample - loss: 2.0453e-04 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200a0afbc48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextCNN\n",
    "\n",
    "model = TextCNN(MAXLEN, vocab_size, embedding_dims)\n",
    "model.compile(optimizer, loss, metrics=metrics)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
