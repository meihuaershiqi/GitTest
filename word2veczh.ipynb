{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于word2vec使用中文wiki语料库训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用的语料库是wiki百科的中文语料库，下载地址：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "* 共365363篇文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "import time\n",
    "import re\n",
    "import zhconv\n",
    "import jieba\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 将xml的wiki数据转换为text格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入wiki数据...\n",
      "wiki数据读入完成！\n",
      "处理程序开始...\n",
      "目前已处理10000条数据，阶段耗时74秒。\n",
      "目前已处理20000条数据，阶段耗时54秒。\n",
      "目前已处理30000条数据，阶段耗时47秒。\n",
      "目前已处理40000条数据，阶段耗时48秒。\n",
      "目前已处理50000条数据，阶段耗时47秒。\n",
      "目前已处理60000条数据，阶段耗时50秒。\n",
      "目前已处理70000条数据，阶段耗时45秒。\n",
      "目前已处理80000条数据，阶段耗时46秒。\n",
      "目前已处理90000条数据，阶段耗时43秒。\n",
      "目前已处理100000条数据，阶段耗时43秒。\n",
      "目前已处理110000条数据，阶段耗时62秒。\n",
      "目前已处理120000条数据，阶段耗时48秒。\n",
      "目前已处理130000条数据，阶段耗时54秒。\n",
      "目前已处理140000条数据，阶段耗时52秒。\n",
      "目前已处理150000条数据，阶段耗时51秒。\n",
      "目前已处理160000条数据，阶段耗时50秒。\n",
      "目前已处理170000条数据，阶段耗时52秒。\n",
      "目前已处理180000条数据，阶段耗时53秒。\n",
      "目前已处理190000条数据，阶段耗时140秒。\n",
      "目前已处理200000条数据，阶段耗时189秒。\n",
      "目前已处理210000条数据，阶段耗时91秒。\n",
      "目前已处理220000条数据，阶段耗时81秒。\n",
      "目前已处理230000条数据，阶段耗时70秒。\n",
      "目前已处理240000条数据，阶段耗时76秒。\n",
      "目前已处理250000条数据，阶段耗时60秒。\n",
      "目前已处理260000条数据，阶段耗时73秒。\n",
      "目前已处理270000条数据，阶段耗时69秒。\n",
      "目前已处理280000条数据，阶段耗时66秒。\n",
      "目前已处理290000条数据，阶段耗时58秒。\n",
      "目前已处理300000条数据，阶段耗时60秒。\n",
      "目前已处理310000条数据，阶段耗时65秒。\n",
      "目前已处理320000条数据，阶段耗时67秒。\n",
      "目前已处理330000条数据，阶段耗时67秒。\n",
      "目前已处理340000条数据，阶段耗时70秒。\n",
      "目前已处理350000条数据，阶段耗时71秒。\n",
      "目前已处理360000条数据，阶段耗时75秒。\n",
      "处理程序结束！\n",
      "Wall time: 40min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'zhwiki-latest-pages-articles.xml.bz2'\n",
    "output_file_name = 'wiki.zh.txt'\n",
    "print('开始读入wiki数据...')\n",
    "input_file = WikiCorpus(input_file_name, lemmatize=False, dictionary={})\n",
    "print('wiki数据读入完成！')\n",
    "\n",
    "with open(output_file_name, 'w', encoding=\"utf-8\") as output_file:\n",
    "    print('处理程序开始...')\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for text in input_file.get_texts():\n",
    "        output_file.write(' '.join(text) + '\\n')\n",
    "        count = count + 1\n",
    "        if count % 10000 == 0:\n",
    "            end = time.time()\n",
    "            print(f'目前已处理{count}条数据，阶段耗时{int(end-start)}秒。')\n",
    "            start = time.time()\n",
    "    print('处理程序结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 去除非中文词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入数据文件...\n",
      "读入数据文件结束！\n",
      "去除非中文词--程序执行开始...\n",
      "目前已分词10000条数据。\n",
      "目前已分词20000条数据。\n",
      "目前已分词30000条数据。\n",
      "目前已分词40000条数据。\n",
      "目前已分词50000条数据。\n",
      "目前已分词60000条数据。\n",
      "目前已分词70000条数据。\n",
      "目前已分词80000条数据。\n",
      "目前已分词90000条数据。\n",
      "目前已分词100000条数据。\n",
      "目前已分词110000条数据。\n",
      "目前已分词120000条数据。\n",
      "目前已分词130000条数据。\n",
      "目前已分词140000条数据。\n",
      "目前已分词150000条数据。\n",
      "目前已分词160000条数据。\n",
      "目前已分词170000条数据。\n",
      "目前已分词180000条数据。\n",
      "目前已分词190000条数据。\n",
      "目前已分词200000条数据。\n",
      "目前已分词210000条数据。\n",
      "目前已分词220000条数据。\n",
      "目前已分词230000条数据。\n",
      "目前已分词240000条数据。\n",
      "目前已分词250000条数据。\n",
      "目前已分词260000条数据。\n",
      "目前已分词270000条数据。\n",
      "目前已分词280000条数据。\n",
      "目前已分词290000条数据。\n",
      "目前已分词300000条数据。\n",
      "目前已分词310000条数据。\n",
      "目前已分词320000条数据。\n",
      "目前已分词330000条数据。\n",
      "目前已分词340000条数据。\n",
      "目前已分词350000条数据。\n",
      "目前已分词360000条数据。\n",
      "去除非中文词--程序执行结束！\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'wiki.zh.txt'\n",
    "output_file_name = 'wiki.cn.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('去除非中文词--程序执行开始...')\n",
    "count = 1\n",
    "cn_reg = '[^\\u4e00-\\u9fa5]+'\n",
    "\n",
    "for line in lines:\n",
    "    line = re.sub(cn_reg, \"\", line)\n",
    "    output_file.write(line + '\\n')\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(f'目前已分词{count}条数据。')\n",
    "print('去除非中文词--程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 繁体转为简体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入繁体文件...\n",
      "读入繁体文件结束！\n",
      "转换程序执行开始...\n",
      "目前已转换10000条数据，阶段耗时17秒。\n",
      "目前已转换20000条数据，阶段耗时13秒。\n",
      "目前已转换30000条数据，阶段耗时12秒。\n",
      "目前已转换40000条数据，阶段耗时9秒。\n",
      "目前已转换50000条数据，阶段耗时9秒。\n",
      "目前已转换60000条数据，阶段耗时9秒。\n",
      "目前已转换70000条数据，阶段耗时6秒。\n",
      "目前已转换80000条数据，阶段耗时8秒。\n",
      "目前已转换90000条数据，阶段耗时7秒。\n",
      "目前已转换100000条数据，阶段耗时7秒。\n",
      "目前已转换110000条数据，阶段耗时6秒。\n",
      "目前已转换120000条数据，阶段耗时6秒。\n",
      "目前已转换130000条数据，阶段耗时9秒。\n",
      "目前已转换140000条数据，阶段耗时5秒。\n",
      "目前已转换150000条数据，阶段耗时7秒。\n",
      "目前已转换160000条数据，阶段耗时5秒。\n",
      "目前已转换170000条数据，阶段耗时8秒。\n",
      "目前已转换180000条数据，阶段耗时5秒。\n",
      "目前已转换190000条数据，阶段耗时4秒。\n",
      "目前已转换200000条数据，阶段耗时6秒。\n",
      "目前已转换210000条数据，阶段耗时5秒。\n",
      "目前已转换220000条数据，阶段耗时9秒。\n",
      "目前已转换230000条数据，阶段耗时7秒。\n",
      "目前已转换240000条数据，阶段耗时7秒。\n",
      "目前已转换250000条数据，阶段耗时7秒。\n",
      "目前已转换260000条数据，阶段耗时7秒。\n",
      "目前已转换270000条数据，阶段耗时7秒。\n",
      "目前已转换280000条数据，阶段耗时7秒。\n",
      "目前已转换290000条数据，阶段耗时7秒。\n",
      "目前已转换300000条数据，阶段耗时5秒。\n",
      "目前已转换310000条数据，阶段耗时8秒。\n",
      "目前已转换320000条数据，阶段耗时6秒。\n",
      "目前已转换330000条数据，阶段耗时8秒。\n",
      "目前已转换340000条数据，阶段耗时5秒。\n",
      "目前已转换350000条数据，阶段耗时6秒。\n",
      "目前已转换360000条数据，阶段耗时6秒。\n",
      "转换程序执行结束！\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'wiki.cn.txt'\n",
    "output_file_name = 'wiki.cn.simple.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入繁体文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入繁体文件结束！')\n",
    "\n",
    "print('转换程序执行开始...')\n",
    "start = time.time()\n",
    "count = 1\n",
    "for line in lines:\n",
    "    output_file.write(zhconv.convert(line, 'zh-hans'))\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        end = time.time()\n",
    "        print(f'目前已转换{count}条数据，阶段耗时{int(end-start)}秒。')\n",
    "        start = time.time()\n",
    "print('转换程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入数据文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZCF\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读入数据文件结束！\n",
      "分词程序执行开始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.060 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前已分词10000条数据，阶段耗时149秒。\n",
      "目前已分词20000条数据，阶段耗时96秒。\n",
      "目前已分词30000条数据，阶段耗时87秒。\n",
      "目前已分词40000条数据，阶段耗时78秒。\n",
      "目前已分词50000条数据，阶段耗时78秒。\n",
      "目前已分词60000条数据，阶段耗时67秒。\n",
      "目前已分词70000条数据，阶段耗时65秒。\n",
      "目前已分词80000条数据，阶段耗时60秒。\n",
      "目前已分词90000条数据，阶段耗时59秒。\n",
      "目前已分词100000条数据，阶段耗时59秒。\n",
      "目前已分词110000条数据，阶段耗时66秒。\n",
      "目前已分词120000条数据，阶段耗时57秒。\n",
      "目前已分词130000条数据，阶段耗时59秒。\n",
      "目前已分词140000条数据，阶段耗时60秒。\n",
      "目前已分词150000条数据，阶段耗时56秒。\n",
      "目前已分词160000条数据，阶段耗时59秒。\n",
      "目前已分词170000条数据，阶段耗时63秒。\n",
      "目前已分词180000条数据，阶段耗时54秒。\n",
      "目前已分词190000条数据，阶段耗时49秒。\n",
      "目前已分词200000条数据，阶段耗时50秒。\n",
      "目前已分词210000条数据，阶段耗时52秒。\n",
      "目前已分词220000条数据，阶段耗时46秒。\n",
      "目前已分词230000条数据，阶段耗时54秒。\n",
      "目前已分词240000条数据，阶段耗时49秒。\n",
      "目前已分词250000条数据，阶段耗时50秒。\n",
      "目前已分词260000条数据，阶段耗时57秒。\n",
      "目前已分词270000条数据，阶段耗时67秒。\n",
      "目前已分词280000条数据，阶段耗时58秒。\n",
      "目前已分词290000条数据，阶段耗时55秒。\n",
      "目前已分词300000条数据，阶段耗时48秒。\n",
      "目前已分词310000条数据，阶段耗时52秒。\n",
      "目前已分词320000条数据，阶段耗时55秒。\n",
      "目前已分词330000条数据，阶段耗时58秒。\n",
      "目前已分词340000条数据，阶段耗时56秒。\n",
      "目前已分词350000条数据，阶段耗时55秒。\n",
      "目前已分词360000条数据，阶段耗时54秒。\n",
      "分词程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "input_file_name = 'wiki.cn.simple.txt'\n",
    "output_file_name = 'wiki.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('分词程序执行开始...')\n",
    "start = time.time()\n",
    "count = 1\n",
    "for line in lines:\n",
    "    output_file.write(' '.join(jieba.cut(line)))\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        end = time.time()\n",
    "        print(f'目前已分词{count}条数据，阶段耗时{int(end-start)}秒。')\n",
    "        start = time.time()\n",
    "print('分词程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换过程开始...\n",
      "转换过程结束！\n",
      "开始保存模型...\n",
      "模型保存结束！\n",
      "Wall time: 38min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'wiki.txt'\n",
    "model_file_name = 'wiki.model'\n",
    "\n",
    "print('转换过程开始...')\n",
    "model = Word2Vec(LineSentence(input_file_name),\n",
    "                 size=400,  # 词向量长度为400\n",
    "                 window=5,\n",
    "                 min_count=5,\n",
    "                 workers=multiprocessing.cpu_count())\n",
    "print('转换过程结束！')\n",
    "\n",
    "print('开始保存模型...')\n",
    "model.save(model_file_name)\n",
    "print('模型保存结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = Word2Vec.load('./wiki.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小猫 ==> \n",
      "\n",
      "小狗 0.7167114019393921\n",
      "猫咪 0.6655534505844116\n",
      "爱犬 0.5953876972198486\n",
      "兔子 0.5944821834564209\n",
      "咕咕 0.594245970249176\n",
      "流浪狗 0.5718818306922913\n",
      "小鸡 0.571489691734314\n",
      "狸猫 0.569256067276001\n",
      "吉娃娃 0.5650903582572937\n",
      "猫 0.5615079402923584\n",
      "- - - - - - - - - - - - - - - - - - - - \n",
      "大猫 ==> \n",
      "\n",
      "宠物猫 0.48416590690612793\n",
      "天竺鼠 0.4796520471572876\n",
      "狸猫 0.466729998588562\n",
      "吉娃娃 0.46487247943878174\n",
      "猎狗 0.4463025629520416\n",
      "小狗 0.44608205556869507\n",
      "博美犬 0.44185078144073486\n",
      "腊肠犬 0.44161030650138855\n",
      "大型犬 0.44143423438072205\n",
      "卷毛 0.44055691361427307\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "example = \"小猫\"\n",
    "word = model.most_similar(example)\n",
    "print(f'{example} ==> \\n')\n",
    "for t in word:\n",
    "    print(t[0],t[1])\n",
    "print(\"- \"*20)\n",
    "example = \"大猫\"\n",
    "word = model.most_similar(example)\n",
    "print(f'{example} ==> \\n')\n",
    "for t in word:\n",
    "    print(t[0],t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 国王 + 男人 = 王后 + 女人  ？ ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = 女人\n",
      "It is amazing!\n"
     ]
    }
   ],
   "source": [
    "man = '男人'\n",
    "woman = '女人'\n",
    "king = '国王'\n",
    "queen = '王后'\n",
    "word = model.most_similar(positive=[king, man],negative=[queen],topn=1)\n",
    "print(f'result = {word[0][0]}')\n",
    "if word[0][0]==woman:\n",
    "    print('It is amazing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "太后\n",
      "0.616728\n",
      "0.12081672\n",
      "Wall time: 61.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(model.doesnt_match(u'太后 妃子 贵人 贵妃 才人'.split()))\n",
    "print(model.similarity(u'书籍',u'书本'))\n",
    "print(model.similarity(u'逛街',u'书本'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2Vec in module gensim.models.word2vec object:\n",
      "\n",
      "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |  \n",
      " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |  \n",
      " |  Once you're finished training a model (=no more updates, only querying)\n",
      " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |  \n",
      " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |  \n",
      " |  Some important attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use `self.wv.__contains__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |      Deprecated. Use `self.wv.accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and scoring, to save memory.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Use only if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace_word_vectors_with_normalized : bool, optional\n",
      " |          If True, forget the original (not normalized) word vectors and only keep\n",
      " |          the L2-normalized word vectors, to save even more memory.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Deprecated. Use `self.wv.init_sims` instead.\n",
      " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
