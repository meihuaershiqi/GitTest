{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于word2vec使用中文wiki语料库训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用的语料库是wiki百科的中文语料库，下载地址：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "* 共365363篇文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "import time\n",
    "import re\n",
    "import zhconv\n",
    "import jieba\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 将xml的wiki数据转换为text格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入wiki数据...\n",
      "wiki数据读入完成！\n",
      "处理程序开始...\n",
      "目前已处理10000条数据，阶段耗时74秒。\n",
      "目前已处理20000条数据，阶段耗时54秒。\n",
      "目前已处理30000条数据，阶段耗时47秒。\n",
      "目前已处理40000条数据，阶段耗时48秒。\n",
      "目前已处理50000条数据，阶段耗时47秒。\n",
      "目前已处理60000条数据，阶段耗时50秒。\n",
      "目前已处理70000条数据，阶段耗时45秒。\n",
      "目前已处理80000条数据，阶段耗时46秒。\n",
      "目前已处理90000条数据，阶段耗时43秒。\n",
      "目前已处理100000条数据，阶段耗时43秒。\n",
      "目前已处理110000条数据，阶段耗时62秒。\n",
      "目前已处理120000条数据，阶段耗时48秒。\n",
      "目前已处理130000条数据，阶段耗时54秒。\n",
      "目前已处理140000条数据，阶段耗时52秒。\n",
      "目前已处理150000条数据，阶段耗时51秒。\n",
      "目前已处理160000条数据，阶段耗时50秒。\n",
      "目前已处理170000条数据，阶段耗时52秒。\n",
      "目前已处理180000条数据，阶段耗时53秒。\n",
      "目前已处理190000条数据，阶段耗时140秒。\n",
      "目前已处理200000条数据，阶段耗时189秒。\n",
      "目前已处理210000条数据，阶段耗时91秒。\n",
      "目前已处理220000条数据，阶段耗时81秒。\n",
      "目前已处理230000条数据，阶段耗时70秒。\n",
      "目前已处理240000条数据，阶段耗时76秒。\n",
      "目前已处理250000条数据，阶段耗时60秒。\n",
      "目前已处理260000条数据，阶段耗时73秒。\n",
      "目前已处理270000条数据，阶段耗时69秒。\n",
      "目前已处理280000条数据，阶段耗时66秒。\n",
      "目前已处理290000条数据，阶段耗时58秒。\n",
      "目前已处理300000条数据，阶段耗时60秒。\n",
      "目前已处理310000条数据，阶段耗时65秒。\n",
      "目前已处理320000条数据，阶段耗时67秒。\n",
      "目前已处理330000条数据，阶段耗时67秒。\n",
      "目前已处理340000条数据，阶段耗时70秒。\n",
      "目前已处理350000条数据，阶段耗时71秒。\n",
      "目前已处理360000条数据，阶段耗时75秒。\n",
      "处理程序结束！\n",
      "Wall time: 40min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'zhwiki-latest-pages-articles.xml.bz2'\n",
    "output_file_name = 'wiki.zh.txt'\n",
    "print('开始读入wiki数据...')\n",
    "input_file = WikiCorpus(input_file_name, lemmatize=False, dictionary={})\n",
    "print('wiki数据读入完成！')\n",
    "\n",
    "with open(output_file_name, 'w', encoding=\"utf-8\") as output_file:\n",
    "    print('处理程序开始...')\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for text in input_file.get_texts():\n",
    "        output_file.write(' '.join(text) + '\\n')\n",
    "        count = count + 1\n",
    "        if count % 10000 == 0:\n",
    "            end = time.time()\n",
    "            print(f'目前已处理{count}条数据，阶段耗时{int(end-start)}秒。')\n",
    "            start = time.time()\n",
    "    print('处理程序结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 去除非中文词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入数据文件...\n",
      "读入数据文件结束！\n",
      "去除非中文词--程序执行开始...\n",
      "目前已分词10000条数据。\n",
      "目前已分词20000条数据。\n",
      "目前已分词30000条数据。\n",
      "目前已分词40000条数据。\n",
      "目前已分词50000条数据。\n",
      "目前已分词60000条数据。\n",
      "目前已分词70000条数据。\n",
      "目前已分词80000条数据。\n",
      "目前已分词90000条数据。\n",
      "目前已分词100000条数据。\n",
      "目前已分词110000条数据。\n",
      "目前已分词120000条数据。\n",
      "目前已分词130000条数据。\n",
      "目前已分词140000条数据。\n",
      "目前已分词150000条数据。\n",
      "目前已分词160000条数据。\n",
      "目前已分词170000条数据。\n",
      "目前已分词180000条数据。\n",
      "目前已分词190000条数据。\n",
      "目前已分词200000条数据。\n",
      "目前已分词210000条数据。\n",
      "目前已分词220000条数据。\n",
      "目前已分词230000条数据。\n",
      "目前已分词240000条数据。\n",
      "目前已分词250000条数据。\n",
      "目前已分词260000条数据。\n",
      "目前已分词270000条数据。\n",
      "目前已分词280000条数据。\n",
      "目前已分词290000条数据。\n",
      "目前已分词300000条数据。\n",
      "目前已分词310000条数据。\n",
      "目前已分词320000条数据。\n",
      "目前已分词330000条数据。\n",
      "目前已分词340000条数据。\n",
      "目前已分词350000条数据。\n",
      "目前已分词360000条数据。\n",
      "去除非中文词--程序执行结束！\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'wiki.zh.txt'\n",
    "output_file_name = 'wiki.cn.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('去除非中文词--程序执行开始...')\n",
    "count = 1\n",
    "cn_reg = '[^\\u4e00-\\u9fa5]+'\n",
    "\n",
    "for line in lines:\n",
    "    line = re.sub(cn_reg, \"\", line)\n",
    "    output_file.write(line + '\\n')\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(f'目前已分词{count}条数据。')\n",
    "print('去除非中文词--程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 繁体转为简体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入繁体文件...\n",
      "读入繁体文件结束！\n",
      "转换程序执行开始...\n",
      "目前已转换10000条数据，阶段耗时17秒。\n",
      "目前已转换20000条数据，阶段耗时13秒。\n",
      "目前已转换30000条数据，阶段耗时12秒。\n",
      "目前已转换40000条数据，阶段耗时9秒。\n",
      "目前已转换50000条数据，阶段耗时9秒。\n",
      "目前已转换60000条数据，阶段耗时9秒。\n",
      "目前已转换70000条数据，阶段耗时6秒。\n",
      "目前已转换80000条数据，阶段耗时8秒。\n",
      "目前已转换90000条数据，阶段耗时7秒。\n",
      "目前已转换100000条数据，阶段耗时7秒。\n",
      "目前已转换110000条数据，阶段耗时6秒。\n",
      "目前已转换120000条数据，阶段耗时6秒。\n",
      "目前已转换130000条数据，阶段耗时9秒。\n",
      "目前已转换140000条数据，阶段耗时5秒。\n",
      "目前已转换150000条数据，阶段耗时7秒。\n",
      "目前已转换160000条数据，阶段耗时5秒。\n",
      "目前已转换170000条数据，阶段耗时8秒。\n",
      "目前已转换180000条数据，阶段耗时5秒。\n",
      "目前已转换190000条数据，阶段耗时4秒。\n",
      "目前已转换200000条数据，阶段耗时6秒。\n",
      "目前已转换210000条数据，阶段耗时5秒。\n",
      "目前已转换220000条数据，阶段耗时9秒。\n",
      "目前已转换230000条数据，阶段耗时7秒。\n",
      "目前已转换240000条数据，阶段耗时7秒。\n",
      "目前已转换250000条数据，阶段耗时7秒。\n",
      "目前已转换260000条数据，阶段耗时7秒。\n",
      "目前已转换270000条数据，阶段耗时7秒。\n",
      "目前已转换280000条数据，阶段耗时7秒。\n",
      "目前已转换290000条数据，阶段耗时7秒。\n",
      "目前已转换300000条数据，阶段耗时5秒。\n",
      "目前已转换310000条数据，阶段耗时8秒。\n",
      "目前已转换320000条数据，阶段耗时6秒。\n",
      "目前已转换330000条数据，阶段耗时8秒。\n",
      "目前已转换340000条数据，阶段耗时5秒。\n",
      "目前已转换350000条数据，阶段耗时6秒。\n",
      "目前已转换360000条数据，阶段耗时6秒。\n",
      "转换程序执行结束！\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'wiki.cn.txt'\n",
    "output_file_name = 'wiki.cn.simple.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入繁体文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入繁体文件结束！')\n",
    "\n",
    "print('转换程序执行开始...')\n",
    "start = time.time()\n",
    "count = 1\n",
    "for line in lines:\n",
    "    output_file.write(zhconv.convert(line, 'zh-hans'))\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        end = time.time()\n",
    "        print(f'目前已转换{count}条数据，阶段耗时{int(end-start)}秒。')\n",
    "        start = time.time()\n",
    "print('转换程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入数据文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZCF\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读入数据文件结束！\n",
      "分词程序执行开始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.060 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前已分词10000条数据，阶段耗时149秒。\n",
      "目前已分词20000条数据，阶段耗时96秒。\n",
      "目前已分词30000条数据，阶段耗时87秒。\n",
      "目前已分词40000条数据，阶段耗时78秒。\n",
      "目前已分词50000条数据，阶段耗时78秒。\n",
      "目前已分词60000条数据，阶段耗时67秒。\n",
      "目前已分词70000条数据，阶段耗时65秒。\n",
      "目前已分词80000条数据，阶段耗时60秒。\n",
      "目前已分词90000条数据，阶段耗时59秒。\n",
      "目前已分词100000条数据，阶段耗时59秒。\n",
      "目前已分词110000条数据，阶段耗时66秒。\n",
      "目前已分词120000条数据，阶段耗时57秒。\n",
      "目前已分词130000条数据，阶段耗时59秒。\n",
      "目前已分词140000条数据，阶段耗时60秒。\n",
      "目前已分词150000条数据，阶段耗时56秒。\n",
      "目前已分词160000条数据，阶段耗时59秒。\n",
      "目前已分词170000条数据，阶段耗时63秒。\n",
      "目前已分词180000条数据，阶段耗时54秒。\n",
      "目前已分词190000条数据，阶段耗时49秒。\n",
      "目前已分词200000条数据，阶段耗时50秒。\n",
      "目前已分词210000条数据，阶段耗时52秒。\n",
      "目前已分词220000条数据，阶段耗时46秒。\n",
      "目前已分词230000条数据，阶段耗时54秒。\n",
      "目前已分词240000条数据，阶段耗时49秒。\n",
      "目前已分词250000条数据，阶段耗时50秒。\n",
      "目前已分词260000条数据，阶段耗时57秒。\n",
      "目前已分词270000条数据，阶段耗时67秒。\n",
      "目前已分词280000条数据，阶段耗时58秒。\n",
      "目前已分词290000条数据，阶段耗时55秒。\n",
      "目前已分词300000条数据，阶段耗时48秒。\n",
      "目前已分词310000条数据，阶段耗时52秒。\n",
      "目前已分词320000条数据，阶段耗时55秒。\n",
      "目前已分词330000条数据，阶段耗时58秒。\n",
      "目前已分词340000条数据，阶段耗时56秒。\n",
      "目前已分词350000条数据，阶段耗时55秒。\n",
      "目前已分词360000条数据，阶段耗时54秒。\n",
      "分词程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "input_file_name = 'wiki.cn.simple.txt'\n",
    "output_file_name = 'wiki.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('分词程序执行开始...')\n",
    "start = time.time()\n",
    "count = 1\n",
    "for line in lines:\n",
    "    output_file.write(' '.join(jieba.cut(line)))\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        end = time.time()\n",
    "        print(f'目前已分词{count}条数据，阶段耗时{int(end-start)}秒。')\n",
    "        start = time.time()\n",
    "print('分词程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换过程开始...\n",
      "转换过程结束！\n",
      "开始保存模型...\n",
      "模型保存结束！\n",
      "Wall time: 38min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file_name = 'wiki.txt'\n",
    "model_file_name = 'wiki.model'\n",
    "\n",
    "print('转换过程开始...')\n",
    "model = Word2Vec(LineSentence(input_file_name),\n",
    "                 size=400,  # 词向量长度为400\n",
    "                 window=5,\n",
    "                 min_count=5,\n",
    "                 workers=multiprocessing.cpu_count())\n",
    "print('转换过程结束！')\n",
    "\n",
    "print('开始保存模型...')\n",
    "model.save(model_file_name)\n",
    "print('模型保存结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = Word2Vec.load('./wiki.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小猫 ==> \n",
      "\n",
      "小狗 0.7167114019393921\n",
      "猫咪 0.6655534505844116\n",
      "爱犬 0.5953876972198486\n",
      "兔子 0.5944821834564209\n",
      "咕咕 0.594245970249176\n",
      "流浪狗 0.5718818306922913\n",
      "小鸡 0.571489691734314\n",
      "狸猫 0.569256067276001\n",
      "吉娃娃 0.5650903582572937\n",
      "猫 0.5615079402923584\n",
      "- - - - - - - - - - - - - - - - - - - - \n",
      "大猫 ==> \n",
      "\n",
      "宠物猫 0.48416590690612793\n",
      "天竺鼠 0.4796520471572876\n",
      "狸猫 0.466729998588562\n",
      "吉娃娃 0.46487247943878174\n",
      "猎狗 0.4463025629520416\n",
      "小狗 0.44608205556869507\n",
      "博美犬 0.44185078144073486\n",
      "腊肠犬 0.44161030650138855\n",
      "大型犬 0.44143423438072205\n",
      "卷毛 0.44055691361427307\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "example = \"小猫\"\n",
    "word = model.most_similar(example)\n",
    "print(f'{example} ==> \\n')\n",
    "for t in word:\n",
    "    print(t[0],t[1])\n",
    "print(\"- \"*20)\n",
    "example = \"大猫\"\n",
    "word = model.most_similar(example)\n",
    "print(f'{example} ==> \\n')\n",
    "for t in word:\n",
    "    print(t[0],t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 国王 + 男人 = 王后 + 女人  ？ ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = 女人\n",
      "It is amazing!\n"
     ]
    }
   ],
   "source": [
    "man = '男人'\n",
    "woman = '女人'\n",
    "king = '国王'\n",
    "queen = '王后'\n",
    "word = model.most_similar(positive=[king, man],negative=[queen],topn=1)\n",
    "print(f'result = {word[0][0]}')\n",
    "if word[0][0]==woman:\n",
    "    print('It is amazing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "太后\n",
      "0.616728\n",
      "0.12081672\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(model.doesnt_match(u'太后 妃子 贵人 贵妃 才人'.split()))\n",
    "print(model.similarity(u'书籍',u'书本'))\n",
    "print(model.similarity(u'逛街',u'书本'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
